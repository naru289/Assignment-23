{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-23/blob/main/M3_AST_23_Caltech_UCSD_Birds_200_Image_Segmentation_C%20Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 23 : Implementation of UNet Architecture using Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "\n",
        "  At the end of the experiment, you will be able to :\n",
        "\n",
        "  * understand the Caltech-UCSD_Birds-200 dataset\n",
        "  * understand and build the Unet model\n",
        "  * train and evaluate the images using UNet model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtSHMKwlbNsX"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We will be using the Caltech-UCSD Birds-200-2011 dataset released by Caltech, it contains around 11,788 images of birds belonging to 200 species along with their masked images.\n",
        "\n",
        "You can download the image dataset and the segmentations from this [link](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html)\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Caltech-birds.png\" width=900px height=350px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "From the above figure, you can see the image of the bird and its mask pair.\n",
        "\n",
        "The CUB-200‚Äì2011 dataset contains images of North American birds from a range of 200 different species. It is a challenging problem as many of the species of birds have degree of visual similarity.\n",
        "\n",
        "The dataset was originally produced in 2010 (CUB-200), and contained ~6000 images of the 200 classes of birds. Accompanying this was additional label data including bounding boxes, rough segmentations and additional attributes. This was updated in 2011 (CUB-200‚Äì2011), to add additional images, bringing the total number of images in the dataset to almost 12,000. The available attributes were also updated to include 15 part locations, 312 binary attributes and a bounding box per image.\n",
        "\n",
        "**Directory Information**\n",
        "\n",
        "- CUB_200_2011/images/\n",
        "    The images organized in subdirectories based on species. See\n",
        "    IMAGES AND CLASS LABELS section below for more info.\n",
        "- CUB_200_2011/parts/\n",
        "    15 part locations per image. See PART LOCATIONS section below\n",
        "    for more info.\n",
        "- CUB_200_2011/attributes/\n",
        "    322 binary attribute labels from MTurk workers. See ATTRIBUTE LABELS\n",
        "    section below for more info.\n",
        "\n",
        "**IMAGES AND CLASS LABELS:**\n",
        "\n",
        "Images are contained in the directory CUB_200_2011/images/, with 200 subdirectories (one for each bird species)\n",
        "\n",
        "**List of image files (images.txt)**\n",
        "\n",
        "The list of image file names is contained in the file images.txt, with each line corresponding to one image:\n",
        "\n",
        "Eg: for the following image 001.Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
        "\n",
        "\n",
        "**image_id** - 001\n",
        "\n",
        "**image_name** - Black_footed_Albatross/Black_Footed_Albatross_0046_18.jpg\n",
        "\n",
        "\n",
        "\n",
        "**Train/test split (train_test_split.txt)**\n",
        "\n",
        "The suggested train/test split is contained in the file train_test_split.txt, with each line corresponding to one image:\n",
        "\n",
        "**image_id** - where image_id corresponds to the ID in images.txt\n",
        "\n",
        "**is_training_image** -  a value of 1 or 0 for is_training_image denotes that the file is in the training or test set, respectively.\n",
        "\n",
        "**Image class labels (image_class_labels.txt)**\n",
        "\n",
        "The ground truth class labels (bird species labels) for each image are contained in the file image_class_labels.txt, with each line corresponding to one image:\n",
        "\n",
        "where image_id and class_id correspond to the IDs in images.txt and classes.txt, respectively.\n",
        "\n",
        "**BOUNDING BOXES:**\n",
        "\n",
        "Each image contains a single bounding box label.  Bounding box labels are contained in the file bounding_boxes.txt, with each line corresponding to one image:\n",
        "\n",
        "where image_id corresponds to the ID in images.txt, and x, y, width, and height are all measured in pixels.\n",
        "\n",
        "For more details about part locations, attribute labels refer to README file.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Before starting building the model, let's implement the data pipeline from which we get appropriate input for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9teMrrQgeb6o"
      },
      "source": [
        "### Building Data Pipeline\n",
        "\n",
        "Let's look at some metadata we got and deduced by looking at the downloaded dataset folders.\n",
        "\n",
        "1. The Bird images and their corresponding mask has the same name but the image is in jpg format and the mask is in png format.\n",
        "\n",
        "2. Each species is separated into its respective folders, but Caltech researchers were kind enough to provide a text file that contains a list of paths to all the images.\n",
        "\n",
        "3. There are other metadata such as bounding boxes and classes but we do not need those data to build this model.\n",
        "\n",
        "4. The Bird images and their segmentations (comes in two separate files)\n",
        "\n",
        "Here we only consider the images , segmentations and the images.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "GXbNUL2L6LoU",
        "outputId": "b46d9afd-ed20-4da9-a9cc-fce66ca4a02d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_23_Caltech-UCSD_Birds-200_Image_Segmentation_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/CUB_200_2011.zip\")\n",
        "    ipython.magic(\"sx unzip CUB_200_2011.zip\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/segmentations.zip\")\n",
        "    ipython.magic(\"sx unzip segmentations.zip\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2237180&recordId=2038\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJgiOvMsfX7L"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrDHm58fx9M-"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "import PIL,glob\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from torch import nn\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O824EqhfcxW"
      },
      "source": [
        "### Create the Bird Dataset with Image and Segmentations\n",
        "\n",
        "\n",
        "First we create a class \"BirdDataset\" which extends the Pytorch's Dataset class.Following three method need to be overloaded.\n",
        "\n",
        "**`__init__:`** This method is where the dataset object would be initialized. Usually, you need to build your image file paths (images.txt file that contains all the path to bird images) and corresponding labels (segmentations) which are mask file paths for segmentation. These paths are then used in the **`__len__`** and **`__getitem__`** method.\n",
        "\n",
        "\n",
        "**`__getitem__:`** This method is called whenever you would use object[index] to access any element. So we need to write the image and mask loading logic here. So in essence, you get one training sample from your dataset using dataset object from this method. we first extracted the image name of the file so that we can append the extension as needed (jpg for images and png for masks) then opened them using PIL, and applied transformations.\n",
        "\n",
        "**`__len__:`** This method is invoked whenever len(obj) would be used. This method simply returns the number of the training samples in the directory.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOW-fr12zpI1"
      },
      "source": [
        "class BirdDataset(Dataset):\n",
        "    def __getitem__(self, index):\n",
        "        image_name = \".\".join(self.images_paths[index].split('.')[:-1])\n",
        "\n",
        "        # Read the images and the corresponding segmentation from the image.txt\n",
        "        image = Image.open(os.path.join(self.image_dir, f\"{image_name}.jpg\")).convert(\"RGB\")\n",
        "        seg = Image.open(os.path.join(self.segmentation_dir, f\"{image_name}.png\")).convert(\"L\")\n",
        "\n",
        "        # Apply transformations on images and the segmentation\n",
        "        image = self.transform_image(image)\n",
        "        seg = self.transform_mask(seg)\n",
        "\n",
        "        return image, seg\n",
        "\n",
        "    def __init__(self, image_paths, image_dir, segmentation_dir, transform_image, transform_mask):\n",
        "        super(BirdDataset, self).__init__()\n",
        "        self.image_dir = image_dir\n",
        "        self.segmentation_dir = segmentation_dir\n",
        "        self.transform_image = transform_image\n",
        "        self.transform_mask = transform_mask\n",
        "        with open(image_paths, 'r') as f:\n",
        "            self.images_paths = [line.split(\" \")[-1] for line in f.readlines()]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVrR9tkZl9fa"
      },
      "source": [
        "### Data loader creation\n",
        "\n",
        "Now that we have the dataset class defined, the next step is to create a PyTorch data loader from this. Data loaders allow you to create batches of data samples and labels using multiprocessing. This makes the data loading process much faster and efficient. The DataLoader class available under torch.utils.data is used for this purpose. You create a DataLoader object by passing the dataset object to it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Pqox09z0Sc"
      },
      "source": [
        "def load_data_set(image_paths, image_dir, segmentation_dir, transforms, batch_size=8, shuffle=True):\n",
        "    # Call the BirdDataset class\n",
        "    dataset = BirdDataset(image_paths,\n",
        "                          image_dir,\n",
        "                          segmentation_dir,\n",
        "                          transform_image=transforms[0],\n",
        "                          transform_mask=transforms[1])\n",
        "\n",
        "    # Randomly split the data in to train and the validation\n",
        "    # Here we are using only 16 images for validating the unet model (you can also increase the size of validation set (80:20 ratio))\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [11772, 16])\n",
        "\n",
        "    # Load the datasets in the batches\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_dataset, val_dataset, train_loader, val_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovwyUp1knz5t"
      },
      "source": [
        "Pytorch's DataLoader class helps us to create batches and randomly shuffle the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPLoWvRbEF0I"
      },
      "source": [
        "### Visualization of Images and the corresponding Segmentations (Masks)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rx9BW_gIPGzd"
      },
      "source": [
        "# Preparing the data for visualization\n",
        "# First we are loading the images.txt which contains the filenames of images and creating a dataframe with paths\n",
        "df = pd.read_csv('/content/CUB_200_2011/CUB_200_2011/images.txt', header=None)\n",
        "df.columns = ['path']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkeo95dokg4l"
      },
      "source": [
        "fig = plt.figure(figsize=(14, 14))\n",
        "# Resize the image to 256x256\n",
        "dsize = (256, 256)\n",
        "columns = 4\n",
        "rows = 4\n",
        "i = 1\n",
        "# Plotting the 8 random images and the corresponding segmentations\n",
        "paths = df.sample(n=8)\n",
        "for idx,img_id in paths.iterrows():\n",
        "  img_path = img_id['path'].split(' ')[1]\n",
        "  # print(img_path)\n",
        "  # Read the images\n",
        "  image = cv2.imread(os.path.join('/content/CUB_200_2011/CUB_200_2011/images', img_path))\n",
        "  # Read the mask iages\n",
        "  mask =  cv2.imread(os.path.join('/content/segmentations', img_path[:-3]+'png'))\n",
        "  # Resize the original and the mask images\n",
        "  image = cv2.resize(image, dsize)\n",
        "  mask = cv2.resize(mask, dsize)\n",
        "  ax1 = fig.add_subplot(rows, columns, i)\n",
        "  i += 1\n",
        "  ax1.imshow(image)\n",
        "  plt.title(img_path.split('.')[1].split('/')[0])\n",
        "  ax1 = fig.add_subplot(rows, columns, i)\n",
        "  i += 1\n",
        "  ax1.imshow(mask)\n",
        "  plt.title(img_path.split('.')[1].split('/')[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68v0cXlYn-zR"
      },
      "source": [
        "### Building UNet Architecture\n",
        "\n",
        "We will be building a UNet model that takes an RGB image and returns a Black and White mask image of the same height and width, exactly like it is in the dataset.\n",
        "\n",
        "We will follow the below digram of UNet architecture.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1200/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=650px height=450px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "You can see why this network got this name, it is kind of shaped like the letter \"U\".\n",
        "\n",
        "A U-shaped architecture consists of a specific encoder-decoder scheme: The encoder reduces the spatial dimensions in every layer and increases the channels. On the other hand, the decoder increases the spatial dims while reducing the channels. The tensor that is passed in the decoder is usually called **bottleneck**. In the end, the spatial dims are restored to make a prediction for each pixel in the input image. These kinds of models are extremely utilized in real-world applications.\n",
        "\n",
        "It can also be divided into an encoder-decoder path or contracting-expansive path equivalently.\n",
        "\n",
        "Encoder (left side): It consists of the repeated application of **two 3x3 convolutions**. Each conv is followed by a ReLU and batch normalization. Then a 2x2 max pooling operation is applied to reduce the spatial dimensions. Again, at each downsampling step, we double the number of feature channels, while we cut in half the spatial dimensions.\n",
        "\n",
        "Decoder path (right side): Every step in the expansive path consists of an upsampling of the feature map followed by a 2x2 transpose convolution, which halves the number of feature channels. We also have a concatenation with the corresponding feature map from the contracting path, and usually a 3x3 convolutional (each followed by a ReLU). At the final layer, a 1x1 convolution is used to map the channels to the desired number of classes.\n",
        "\n",
        "We will deal with them one by one, but before that, you can notice that each layer has one thing in common, there is an input layer that is followed by two same convolution layers, which means that the convolution does not change the height and width of the image only the number of channel is changed. So, we will implement that **double convolution module (DoubleConv)**.\n",
        "\n",
        "Each block takes an input applies two 3X3 convolution layers followed by a 2X2 max pooling. The number of kernels or feature maps after each block doubles so that architecture can learn the complex structures effectively. The bottommost layer mediates between the contraction layer and the expansion layer. It uses two 3X3 CNN layers followed by 2X2 up convolution layer.\n",
        "\n",
        "For more details refer to the following [link](https://towardsdatascience.com/unet-line-by-line-explanation-9b191c76baf5)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63vDJAsm2sIf"
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernal_size, strides, padding):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernal_size, strides, padding, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernal_size, strides, padding, bias=False),\n",
        "            nn.BatchNorm2d(num_features=out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# In the __init__ method we take in the in_channels which will be 3 in our case.\n",
        "# And then we need the number of segmentation (which is 1) and a list of features in each layer of upsampling and downsampling.\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, num_segmentations=1, features=[64, 128, 256, 512]):\n",
        "        super(UNet, self).__init__()\n",
        "        # we declare the module list that will hold the down layers and up layers\n",
        "        self.ups = nn.ModuleList()\n",
        "        self.downs = nn.ModuleList()\n",
        "        # Let's also define other layers, bottleneck, max pool and output layer before we start populating the ups and downs list\n",
        "        self.bottleneck = DoubleConv(\n",
        "            in_channels=features[-1],\n",
        "            out_channels=features[-1]*2,\n",
        "            kernal_size=3,\n",
        "            strides=1,\n",
        "            padding=1\n",
        "        )\n",
        "        self.output = nn.Conv2d(\n",
        "            in_channels=features[0],\n",
        "            out_channels=num_segmentations,\n",
        "            kernel_size=1\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # For the downward layers, we can just loop through the features and append our DoubleConv module\n",
        "        # because the first layer will come from the input image or from the previous max pool layer.\n",
        "        in_channels_iter = in_channels\n",
        "        for feature in features:\n",
        "            self.downs.append(DoubleConv(\n",
        "                    in_channels=in_channels_iter,\n",
        "                    out_channels=feature,\n",
        "                    kernal_size=3,\n",
        "                    strides=1,\n",
        "                    padding=1\n",
        "                ))\n",
        "            in_channels_iter = feature\n",
        "\n",
        "        # As for the up layers, we will be using a Transpose Convolution layer to upsample the tensors and after that, we will add the DoubleConv layer.\n",
        "        # But in this case, we have to loop through the feature in reverse order.\n",
        "        for feature in reversed(features):\n",
        "            up = nn.Sequential(\n",
        "                nn.ConvTranspose2d(\n",
        "                    in_channels=feature*2,\n",
        "                    out_channels=feature,\n",
        "                    kernel_size=2,\n",
        "                    stride=2,\n",
        "                    padding=0\n",
        "                ),\n",
        "                DoubleConv(\n",
        "                    in_channels=feature*2,\n",
        "                    out_channels=feature,\n",
        "                    kernal_size=3,\n",
        "                    padding=1,\n",
        "                    strides=1\n",
        "                )\n",
        "            )\n",
        "\n",
        "            self.ups.append(up)\n",
        "\n",
        "    # First, we send the image down the UNet through Downward layers and we saved the output of DoubleConv before applying the max pool.\n",
        "    def forward(self, x):\n",
        "        skip_connections = []\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skip_connections.append(x)\n",
        "            x = self.pool(x)\n",
        "\n",
        "        # After that, we send in the tensor through the bottleneck layer and prepare the skip_connections to be concatenated with the up layers by reversing it.\n",
        "        x = self.bottleneck(x)\n",
        "        skip_connections = skip_connections[::-1]\n",
        "\n",
        "        for i in range(len(self.ups)):\n",
        "            x = self.ups[i][0](x) # Pass through ConvTranspose first\n",
        "\n",
        "            skip_connection = skip_connections[i]\n",
        "\n",
        "            # If the height and width of output tensor and skip connection\n",
        "            # is not same then resize the tensor\n",
        "            if x.shape != skip_connection.shape:\n",
        "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "            # Concat the output tensor with skip connection\n",
        "            concat_x = torch.cat((skip_connection, x), dim=1)\n",
        "\n",
        "            # Pass the concatenated tensor through DoubleConv\n",
        "            x = self.ups[i][1](concat_x)\n",
        "\n",
        "        # Pass it through the output layer\n",
        "        return self.output(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDuax8uMtyTF"
      },
      "source": [
        "#### Next, we'll define the Hyperparameters and configs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhSFYTky3AFA"
      },
      "source": [
        "config = {\n",
        "    \"lr\": 1e-3,\n",
        "    \"batch_size\": 16,\n",
        "    \"image_dir\": \"/content/CUB_200_2011/CUB_200_2011/images\",\n",
        "    \"segmentation_dir\": \"/content/segmentations\",\n",
        "    \"image_paths\": \"/content/CUB_200_2011/CUB_200_2011/images.txt\",\n",
        "    \"epochs\": 3, # Increase the number of epochs to see the improvement in the predicted images\n",
        "    \"checkpoint\": \"/content/bird_segmentation_v1.pth\",\n",
        "    \"optimiser\": \"/content/bird_segmentation_v1_optim.pth\",\n",
        "    \"continue_train\": False,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paLEQm6vD71Z"
      },
      "source": [
        "### Define Transformations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUDQifFo4I0E"
      },
      "source": [
        "transforms_image = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0., 0., 0.), (1., 1., 1.))\n",
        "])\n",
        "\n",
        "transforms_mask = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.,), (1.,))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz43rU_F4JRt"
      },
      "source": [
        "# Call the load_data_set function to get the data loaders and the datasets\n",
        "train_dataset, val_dataset, train_loader, val_loader = load_data_set(\n",
        "    config['image_paths'],\n",
        "    config['image_dir'],\n",
        "    config['segmentation_dir'],\n",
        "    transforms=[transforms_image, transforms_mask],\n",
        "    batch_size=config['batch_size']\n",
        ")\n",
        "\n",
        "print(\"loaded\", len(train_loader), \"batches\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKOG3NhbRQDI"
      },
      "source": [
        "# Length of the trainset\n",
        "print(len(train_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmiIbK-Q4Q8p"
      },
      "source": [
        "# UNet model\n",
        "model = UNet(in_channels=3).to(config['device'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71ZV2nCg4gqL"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oc63NyKy4dIF"
      },
      "source": [
        "# We can also use the pretrained model\n",
        "if config['continue_train']:\n",
        "    state_dict = torch.load(config['checkpoint'])\n",
        "    optimiser_state = torch.load(config['optimiser'])\n",
        "    model.load_state_dict(state_dict)\n",
        "    optimiser.load_state_dict(optimiser_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID7vFkE2yW0X"
      },
      "source": [
        "### Defining Loss Function and Optimizer\n",
        "\n",
        "We will be using the Binary Crossentropy loss because we are dealing with binary segmentation here. And also, we will be using Pytorch's Automated Mixed Precision library to automatically set the precision of the gradients. This will reduce the VRAM consumed and also sped up the learning process.\n",
        "\n",
        "We need to use BCE with Logits instead of normal BCE error because we are not using a sigmoid in the model architecture. BCE with Logits will pass the tensor through a sigmoid function before calculating the loss.\n",
        "\n",
        "In case you want to segment more than one items like in a photo you want to segment people, cars, trees, sky, etc. then you can simply change the num_segmentations parameter in the UNet class and change the loss function to Cross-Entropy Loss torch.nn.CrossEntropyLoss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luVT5yiu4qXg"
      },
      "source": [
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "optimiser = torch.optim.Adam(params=model.parameters(), lr=config['lr'])\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxn9LU6ynD_n"
      },
      "source": [
        "# Create a test directory to save the predictions and the ground truths\n",
        "!mkdir /content/test\n",
        "!mkdir test/true"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw-XdWGfrpg6"
      },
      "source": [
        "!mkdir test/pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Dev-NdzA0Q"
      },
      "source": [
        "### Evaluation Metrics - Dice Coefficient\n",
        "\n",
        "The Dice score is often used to quantify the performance of image segmentation methods. There you annotate some ground truth region in your image and then make an automated algorithm to do it. You validate the algorithm by calculating the Dice score, which is a measure of how similar the objects are. So it is the size of the overlap of the two segmentations divided by the total size of the two objects. Using the same terms as describing accuracy, the Dice score is:\n",
        "\n",
        "$ = \\frac{2 \\ . \\ number \\ of \\ true \\ positives}{2 \\ ‚ãÖ \\ number \\ of \\ true \\ positives \\ + \\ number \\ of \\ false \\ positives \\ + \\ number \\ of \\ false \\ negatives} $\n",
        "\n",
        "For a measure of accuracy we will be using Dice Score also know as F1 Score because if we use normal pixel accuracy since 80% of the mask image is black, the model can get an accuracy of 80% just by generating a black screen every time.\n",
        "\n",
        "The Dice score is not only a measure of how many positives you find, but it also penalizes for the false positives that the method finds, similar to precision. so it is more similar to precision than accuracy. The only difference is the denominator, where you have the total number of positives instead of only the positives that the method finds. So the Dice score is also penalizing for the positives that your algorithm/method could not find.\n",
        "\n",
        "**For Eg:** In the case of image segmentation, let's say that you have a mask with ground truth, let's call the mask ùê¥. So the mask has values 1 in the pixels where there is something you are trying to find and else zero. Now you have an algorithm to generate image/mask ùêµ, which also has to be a binary image, i.e. you segmentation images (mask). Then we have the following:\n",
        "\n",
        "\n",
        "*   **Number of positives** is the total number of pixels that have intensity 1 in image ùê¥\n",
        "\n",
        "*   **Number of true positives** is the total number of pixels which have the value 1 in both ùê¥\n",
        "and ùêµ. So it the intersection of the regions of ones in ùê¥ and ùêµ. It is the same as using the AND operator on ùê¥ and ùêµ.\n",
        "\n",
        "*   **Number of false positives** is the number of pixels which appear as 1 in ùêµ but zero in ùê¥\n",
        "<br><br>\n",
        "<center>\n",
        "\n",
        "   **Dice coefficient = $\\frac{2‚ãÖ|ùê¥‚à©ùêµ|}{|ùê¥|+|ùêµ|}$**\n",
        "\n",
        "The idea is simple we count the similar pixels (taking intersection, present in both the images) in the both images we are comparing and multiple it by 2. And divide it by the total pixels in both the images. The below diagrams will make the picture more clear.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Dice_Coefficient.png\" width=400px height=300px/>\n",
        "</center>\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMsNMALC48-2"
      },
      "source": [
        "def check_accuracy_and_save(model, optimiser, epoch):\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), config['checkpoint'])\n",
        "    torch.save(optimiser.state_dict(), config['optimiser'])\n",
        "\n",
        "    num_correct = 0\n",
        "    num_pixel = 0\n",
        "    dice_score = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Loop through the validation datset\n",
        "        for x, y in val_loader:\n",
        "            x = x.to(config['device'])\n",
        "            y = y.to(config['device'])\n",
        "\n",
        "            # Apply sigmoid to predicted out if not applied earlier\n",
        "            preds = torch.sigmoid(model(x))\n",
        "\n",
        "            # For predicted outputs make all values above 0.5 probability as 1 and rest 0\n",
        "            preds = (preds > 0.5).float()\n",
        "\n",
        "            # For calculating the Dice Score for every class we multiply the (prediction * target) and divide by the sum of (predictions and target)\n",
        "            dice_score += (2 * (preds * y).sum()) / (\n",
        "                (preds + y).sum() + 1e-8\n",
        "            )\n",
        "\n",
        "            # torchvision.utils.save_image(preds, f\"/content/test/pred/{epoch}.png\")\n",
        "            # torchvision.utils.save_image(y, f\"/content/test/true/{epoch}.png\")\n",
        "\n",
        "            # Save the target and the predicted images for each epoch\n",
        "            for i in range(preds.size(0)):\n",
        "              torchvision.utils.save_image(preds[i, :, :, :],f\"/content/test/pred/{i}.png\")\n",
        "            for i in range(y.size(0)):\n",
        "              torchvision.utils.save_image(y[i, :, :, :], f\"/content/test/true/{i}.png\")\n",
        "\n",
        "    # Print the dice score\n",
        "    print(f\"Dice Score = {dice_score/len(val_loader)}\")\n",
        "\n",
        "    # Visualize the target and the predicted images\n",
        "    j = 1\n",
        "    columns = 4\n",
        "    rows = 8\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "    for i in glob.glob(\"/content/test/true/*.png\"):\n",
        "      im = np.array(PIL.Image.open(i))\n",
        "      mask = np.array(PIL.Image.open(\"/content/test/pred/\"+os.path.basename(i)))\n",
        "      ax1 = fig.add_subplot(rows, columns, j)\n",
        "      ax1.imshow(im)\n",
        "      plt.tight_layout(pad=0.5)\n",
        "      plt.title(f\"{epoch} Ground Truth : {os.path.basename(i)}\")\n",
        "      ax1 = fig.add_subplot(rows, columns, j+1)\n",
        "      ax1.imshow(mask)\n",
        "      plt.title(f\"{epoch} Predicted Mask : {os.path.basename(i)}\")\n",
        "      j += 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqebyS17ys4p"
      },
      "source": [
        "### Training the UNet model\n",
        "\n",
        "\n",
        "In the train function, we create two loops one for epochs and the other for the batches. We use the autocast wrapper to automatically cast the gradients to float16 or float32 as required and we just update the weights using Pytorch abstraction.\n",
        "\n",
        "For 3 epochs the training takes around 1hr to complete the execution and also to see the better results try increasing the number of epochs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLSHHAiz5J8a"
      },
      "source": [
        "def train():\n",
        "    step = 0\n",
        "    for epoch in range(config['epochs']):\n",
        "\n",
        "        # Loop through the train dataset\n",
        "        loop = tqdm(train_loader)\n",
        "\n",
        "        for image, seg in loop:\n",
        "            # Get the image and the corresponding mask\n",
        "            image = image.to(config['device'])\n",
        "            seg = seg.float().to(config['device'])\n",
        "\n",
        "            with torch.cuda.amp.autocast():\n",
        "                # Do the forward pass\n",
        "                # Get the predictions from the model\n",
        "                pred = model(image)\n",
        "                # Compute the loss between the target and the predictions\n",
        "                loss = loss_fn(pred, seg)\n",
        "\n",
        "            # Zero out the gradients\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "            # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
        "            # Backward ops run in the same dtype autocast chose for corresponding forward ops\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "            # If these gradients do not contain infs or NaNs, optimizer.step() is then called\n",
        "            # otherwise, optimizer.step() is skipped\n",
        "            scaler.step(optimiser)\n",
        "\n",
        "            # Updates the scale for next iteration.\n",
        "            scaler.update()\n",
        "\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "            step += 1\n",
        "        # Call the above function and plot the predicted and the ground truth masks\n",
        "        check_accuracy_and_save(model, optimiser, epoch)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHDYadgG-RHl",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Which of the following is true for semantic segmentation?\n",
        "Answer1 = \"All of the above\" #@param [\"\",\"Semantic Segmentation can be considered as pixel wise classification problem i.e., a class label is supposed to be assigned to each pixel\", \"Semantic segmentation is not limited to two categories. The number of categories for classifying the content of the image can be changed.\", \"It has applications in Autonomous driving (for identifying a drivable path for cars by separating the road from obstacles like pedestrians), Industrial inspection, Medical imaging analysis\", \"All of the above\"]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRwvwQ8_VbWR"
      },
      "source": [
        "#### Consider the following statements about U-Net and answer Q2.\n",
        "\n",
        "A. A U-Net is a fully convolutional neural network architecture that was developed for biomedical image segmentation.\n",
        "\n",
        "B. The loss function for U-Net is a pixel-wise softmax with cross entropy.\n",
        "\n",
        "C. U-Nets have been found to be very effective for tasks where the output is of similar size as the input and the output needs that amount of spatial resolution, and this makes them very good for creating segmentation masks.\n",
        "\n",
        "D. The network architecture of a U-NET consists of a contracting path and an expansive path and in total the network has 23 convolutional layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_F3RfGojqZDc"
      },
      "source": [
        "#@title Q.2. Which of the above statements is/are True regarding U-Net?\n",
        "Answer2 = \"A, B, C and D\" #@param [\"\",\"Only A\", \"Only C\", \"Only D\", \"Both A and B\",\"Both C and D\", \"Both B and D\", \"A, B, C and D\"]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "outputId": "6c24759d-fb01-462d-93de-661413380182",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2038\n",
            "Date of submission:  01 Jul 2023\n",
            "Time of submission:  16:39:59\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}